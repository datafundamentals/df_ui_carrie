/*
Copyright 2014 <%= datasource.authorName %>

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
 
This file generated by <%= datasource.authorName %> via 'Carrie' UI
 */


package com.datafundamentals.refactorme;

import org.apache.camel.builder.RouteBuilder;

/**
 * A Camel Java DSL Router - generated by <%= datasource.authorName %> via 'Carrie' UI
 */
public class MyRouteBuilder extends RouteBuilder {
     
    public void configure() {
// set the ftp target directory, also first use of ssh to set known_hosts file which seems to be required for sftp to work later
      from("timer:foo?repeatCount=1")
      .setBody(constant("mkdir -p <%= datasource.ftpTargetDir %>/<%= datasource.uniqueName %>/<%= datasource.tableName %>"))
      .to("log:org.apache.camel.builder.MyRouteBuilder?showAll=true&multiline=true")
      .to("ssh://<%= datasource.hadoopVmUserName %>@<%= datasource.hadoopVmSshIpAddress %>?password=<%= datasource.hadoopVmPassword %>&useFixedDelay=true&delay=5000");
      
// copy any data files which appear in the ETL drop folder to processing folder to run toAvro
        from("file:<%= datasource.serverHome %>/<%= datasource.etlDropFolder %>/<%= datasource.uniqueName %>?noop=true")
        .log("copied file for avro transformation: ${in.header.CamelFileName}")
		.to("file:<%= datasource.serverHome %>/<%= datasource.etlDropFolder %>/<%= datasource.uniqueName %>/transform");

// run toAvro and create new avro files
        from("file:<%= datasource.serverHome %>/<%= datasource.etlDropFolder %>/<%= datasource.uniqueName %>/transform?include=.*.csv")
        .log("toAvro transformation: ${in.header.CamelFileName} should have created a file at <%= datasource.serverHome %>/<%= datasource.etlDropFolder %>/<%= datasource.uniqueName %>/transformed/")
		.to("avroetl:<%= datasource.uniqueName %>?outputFilePath=<%= datasource.serverHome %>/<%= datasource.etlDropFolder %>/<%= datasource.uniqueName %>/transformed/<%= datasource.tableName %>&delimiter=,&exceptionOnBadData=false&className=<%= datasource.className %>&namespace=<%= datasource.packageName %>");

// copy all files to the hadoop server input folder
        from("file:<%= datasource.serverHome %>/<%= datasource.etlDropFolder %>/<%= datasource.uniqueName %>/transformed")
        .log("moved file after avro transformation: ${in.header.CamelFileName} from <%= datasource.serverHome %>/<%= datasource.etlDropFolder %>/<%= datasource.uniqueName %>/transformed/ to <%= datasource.hadoopVmSshIpAddress %>:<%= datasource.sshPort %>/<%= datasource.ftpTargetDir %>/<%= datasource.uniqueName %>")
        .to("sftp://<%= datasource.hadoopVmSshIpAddress %>:<%= datasource.sshPort %>/<%= datasource.ftpTargetDir %>/<%= datasource.uniqueName %>/<%= datasource.tableName %>?password=<%= datasource.hadoopVmPassword %>&username=<%= datasource.hadoopVmUserName %>&knownHostsFile=<%= datasource.serverHome %>/.ssh/known_hosts");	
        
// move the program files too - these should only move first time, after that this should not have any effect
        from("file:bin?noop=true&include=run.*")
        .to("sftp://<%= datasource.hadoopVmSshIpAddress %>:<%= datasource.sshPort %>/<%= datasource.ftpTargetDir %>/<%= datasource.uniqueName %>/<%= datasource.tableName %>?password=<%= datasource.hadoopVmPassword %>&username=<%= datasource.hadoopVmUserName %>&knownHostsFile=<%= datasource.serverHome %>/.ssh/known_hosts");	
        
// run hive commands to inject data from the hadoop vm into the hive tables
        from("timer:foo?delay=10000&period=10000")
        .setBody(constant("source <%= datasource.ftpTargetDir %>/<%= datasource.uniqueName %>/<%= datasource.tableName %>/run.sh"))
        .to("ssh://<%= datasource.hadoopVmUserName %>@<%= datasource.hadoopVmSshIpAddress %>?password=<%= datasource.hadoopVmPassword %>&useFixedDelay=true&delay=5000")
      	.log("ran run.sh for run.hive");
    }
}
